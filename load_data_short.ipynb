{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12d87b30",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "This notebook loads and preproceses all necessary data, namely the following.\n",
    "* OpenWebTextCorpus: for base DistilBERT model\n",
    "* SQuAD datasrt: for Q&A\n",
    "* Natural Questions (needs to be downloaded externally but is preprocessed here): for Q&A\n",
    "* HotPotQA: for Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c82d7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1737f219",
   "metadata": {},
   "source": [
    "## Distilbert Data\n",
    "In the following, we download the english openwebtext dataset from huggingface (https://huggingface.co/datasets/openwebtext). The dataset is provided by Aaron Gokaslan and Vanya Cohen from Brown University (https://skylion007.github.io/OpenWebTextCorpus/).\n",
    "\n",
    "We first load the data, investigate the structure and write the dataset into files of each 10 000 texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cce7623c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 21/21 [00:00<00:00, 2401.45it/s]\n",
      "Generating train split: 100%|██████████| 8013769/8013769 [21:29<00:00, 6215.85 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"openwebtext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "678a5e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 8013769\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have a text-only training dataset with 8 million entries\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b141bce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create necessary folders\n",
    "os.mkdir('data')\n",
    "os.mkdir('data/original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca94f995",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8013769/8013769 [05:00<00:00, 26708.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# save text in chunks of 10000 samples\n",
    "text = []\n",
    "ind = 0\n",
    "\n",
    "for sample in tqdm(ds['train']):\n",
    "    # replace all newlines\n",
    "    sample = sample['text'].replace('\\n','')\n",
    "    \n",
    "    # append cleaned sample to all texts\n",
    "    text.append(sample)\n",
    "    \n",
    "    # if we processed 10000 samples, write them to a file and start over\n",
    "    if len(text) == 10000:\n",
    "        with open(f\"data/original/text_{ind}.txt\", 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(text))\n",
    "        text = []\n",
    "        ind += 1\n",
    "\n",
    "# write remaining samples to a file\n",
    "with open(f\"data/original/text_{ind}.txt\", 'w', encoding='utf-8') as f:\n",
    "    f.write('\\\\n'.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f131dcfc",
   "metadata": {},
   "source": [
    "### Testing\n",
    "If we load the first file, we should get a file that is 10000 lines long and has one column\n",
    "\n",
    "As we do not preprocess the data in any way, but just write the read text into the file, this is all testing necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df50af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/original/text_0.txt\", 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "lines = pd.DataFrame(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ddb0085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed\n"
     ]
    }
   ],
   "source": [
    "assert lines.shape==(10000,1)\n",
    "print(\"Passed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "85bf9c14e9ba73b783ed1274d522bec79eb0b2b739090180d8ce17bb11aff4aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
